<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=â€œwidth=900>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="img/clover.png">
  <title>Songyao Jiang's Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Songyao Jiang</name>
        </p>
        <p>I am a PhD candidate at <a href="http://www.northeastern.edu/">Northeastern University</a>, 
          where I work on computer vision and machine learning in 
          <a href="https://web.northeastern.edu/smilelab/">SmileLab</a> 
          advised by <a href="http://www1.ece.neu.edu/~yunfu/">Dr. Yun (Raymond) Fu</a>. 
        </p> 
        <p>I am a co-founder of an AI beauty startup company <a href="https://www.giaran.com/">Giaran, Inc.</a>, 
          which was acquired by <a href="https://www.shiseido.com/">Shiseido Americas</a> 
          in Nov. 2017 (<a href="https://www.businesswire.com/news/home/20171107006109/en/Shiseido-Americas-Acquires-Giaran/">Here's the News</a>). 
        </p>
        <p>I received my masters degree at the <a href="https://www.umich.edu/">University of Michigan</a> 
          and my bachelors at <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>.
          <!-- At Google I've worked on <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="http://googleresearch.blogspot.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://research.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. -->
        </p>
        <p>I am a photography lover, and here is my <a href="http://gallery.songyaojiang.com">Small Gallery</a>
        </p>
        <!-- <p>
          I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="https://www.nasa.gov/ames">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
        </p> -->
        <p align=center>
          <a href="mailto:jiangsongyao@gmail.com">Email</a> &nbsp/&nbsp
          <a href="doc/SongyaoJiang-CV.pdf">CV</a> &nbsp/&nbsp
          <!-- <a href="doc/SongyaoJiang-bio.txt">Biography</a> &nbsp/&nbsp -->
          <a href="https://github.com/jackyjsy">GitHub</a> &nbsp/&nbsp
          <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
          <a href="https://www.linkedin.com/in/songyao-jiang-739816149/"> LinkedIn </a>
        </p>
        </td>
        <td width="33%">
        <img src="img/songyao_jiang.png" width="100%">
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
          I'm interested in computer vision, machine learning, image processing, and computational photography. Much of my research is about human faces and pose estimation.
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		
    <tr>
      <td width="30%">
          <a href="img/pose_estimate.jpg"><img src='img/pose_estimate.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <papertitle>Video-based Multi-person Pose Estimation and Tracking</papertitle><br>
          <strong>Songyao Jiang</strong>, and Yun Fu<br>
          <em>Current Work</em>, 2019<br>
          Paper / 
          <a href="https://github.com/jackyjsy/MultiPoseInferene">GitHub</a><br>
          <p> Video-based Multi-person Pose Estimation and Tracking. Under development and construction. 
              Inferencing model provided on GitHub.
          </p>
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/image_generation.png"><img src='img/image_generation.png' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
        <!-- <p><a href="https://"> -->
          <papertitle>Spatially Constrained Generative Adversarial Networks for Conditional Image Generation</papertitle><br>
          <strong>Songyao Jiang</strong>, Hongfu Liu, Yue Wu and Yun Fu<br>
          <em>Submitted to a Springer Journal (under review)</em>, 2018<br>
          Paper / 
          <a href="https://github.com/jackyjsy/SCGAN">GitHub</a><br>
          <p>Image generation has raised tremendous attention
              in both academic and industrial areas, especially
              for criminal portrait and fashion design.
              The current studies always focus on
              class labels as the condition where spatial contents are
              randomly generated. The edge details
              and spatial information is usually blurred and difficult 
              to preserve. In light of this, we propose a novel
              Spatially Constrained Generative Adversarial Network
              , which decouples the spatial constraints from
              the latent vector and makes them feasible as
              additional controllable signals. Experimentally, we provide
              both visual and quantitive results, and demonstrate that the proposed SCGAN 
              is very effective in controlling the spatial
              contents as well as generating high-quality images.</p>
          
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/image_translation_3.jpg"><img src='img/image_translation_3.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/1901.01569">
          <papertitle>Segmentation Guided Image-to-Image Translation with Adversarial Networks</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Zhiqiang Tao and Yun Fu<br>
          <em>Submitted to an IEEE International Conference (under review)</em>, 2018<br>
          <a href="https://arxiv.org/abs/1901.01569">Paper</a> / 
          <a href="https://github.com/jackyjsy/SGGAN">GitHub</a><br>
          <p> Recently image-to-image translation methods neglect to 
              utilize higher-level and instance-specific
              information to guide the training process, leading to a great
              deal of unrealistic generated images of low quality. Existing
              methods also lack of spatial controllability during translation.
              To address these challenge, we propose a novel Segmentation
              Guided Generative Adversarial Networks, which
              leverages semantic segmentation to further boost the generation
              performance and provide spatial mapping. Experimental results on multi-domain
              face image translation task empirically demonstrate our ability
              of the spatial modification and our superiority in image quality
              over several state-of-the-art methods. </p>
      </td>
    </tr>
    <tr>
      <td width="30%">
        <a href="img/rule-based.jpg"><img src='img/rule-based.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://ieeexplore.ieee.org/document/7961759">
          <papertitle> Rule-Based Facial Makeup Recommendation System </papertitle></a><br>
          Taleb Alashkar, <strong>Songyao Jiang</strong> and Yun Fu<br>
          <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</em>, 2017 <br>
          <a href="https://ieeexplore.ieee.org/document/7961759">Paper</a> / 
          GitHub<br>
          <p>Facial makeup style plays a key role in the facial appearance making it 
            more beautiful and attractive. Choosing the best makeup style for a certain face 
            to fit a certain occasion is a full art. To solve this problem computationally, 
            an automatic and smart facial makeup recommendation 
            and synthesis system is proposed in this paper. Additionally, an automatic facial 
            makeup synthesis system is developed to apply the recommended style 
            on the facial image as well. To this end, a new dataset with 961 different females photos 
            collected and labeled. </p>
      </td>
    </tr>
    <tr>
      <td width="30%">
        <a href="img/makeup-recommend.png"><img src='img/makeup-recommend.png' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14773">
          <papertitle>  Examples-Rules Guided Deep Neural Network for Makeup Recommendation</papertitle></a><br>
          Taleb Alashkar, <strong>Songyao Jiang</strong>, Shuyang Wang and Yun Fu<br>
          <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2017 <br>
          <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14773">Paper</a> / 
          GitHub<br>
          <p> We consider a fully automatic makeup recommendation system and propose a novel 
            examples-rules guided deep neural network approach. The framework consists of 
            three stages. First, makeup-related facial traits are classified into structured 
            coding. Second, these facial traits are fed in- to examples-rules guided deep 
            neural recommendation model which makes use of the pairwise of Before-After images 
            and the makeup artist knowledge jointly. Finally, to visualize the recommended makeup 
            style, an automatic makeup synthesis system is developed as well. </p>
      </td>
    </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          This website is generated using source code from <a target="_blank" href="https://jonbarron.info/"><strong>Jon Barron</strong></a>.
	        </font>
        </p>
        </td>
      </tr>
      </table>
      <table width="10%" align="center" border="0" cellspacing="0" cellpadding="20">
        <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=C02jbGLeKNPfv3nMc0a1D-t73HzskgbBQUPgjzvtT4c"></script>
      </table>

      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>
  </body>
</html>
