<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-132141027-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-132141027-1');
    </script>
    <meta name=viewport content="width=device-width">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <link rel="icon" type="image/png" href="img/clover.png">
    <title>Songyao Jiang's Homepage</title>
    <!-- <meta http-equiv="Content-Type" content="text/html; charset=us-ascii"> -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type='text/css'>
    <link href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" rel="stylesheet" type='text/css'>
    <link href="css/font.css" rel="stylesheet" type="text/css">
    <!-- <link href="font/stylesheet.css" rel="stylesheet"> -->
    <!-- main stylesheet -->
    <link href="css/main.css" rel="stylesheet" type="text/css">
  </head>

  <body>
    <div class="sidenav">
      <img src="img/clover.png" align="center" width="30%" alt="clover">
      <chinesename>姜淞耀</chinesename>
      <a href="#about">About</a>
      <a href="#news">News</a>
      <a href="#research">Research</a>
      <a href="#pub">Publications</a>
      <a href="#awards">Awards</a>
      <a href="#service">Service</a>
      <p>
        <a href="mailto:jiangsongyao@gmail.com"><i class="fas fa-fw fa-envelope-square"></i></a>
        <a href="doc/CV_Songyao_Jiang_202302.pdf" target="_blank"><i class="ai fa-fw ai-cv-square"></i></a>
        <a href="https://scholar.google.com/citations?user=Q5E2uoMAAAAJ" target="_blank"><i class="fas fa-fw fa-graduation-cap"></i></a>
        <a href="https://github.com/jackyjsy" target="_blank"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
        <a href="https://www.linkedin.com/in/songyao-jiang-739816149/" target="_blank"><i class="fab fa-fw fa-linkedin"></i></a> 
      </p>
    </div>
    <div class="topnav">
      <img src="img/clover.png" width="20px" alt="clover">
      <chinesename> 姜淞耀</chinesename>
      <a href="#about">About</a>
      <a href="#news">News</a>
      <a href="#research">Research</a>
      <a href="#pub">Publications</a>
      <a href="#awards">Awards</a>
      <!-- <a href="#service">Service</a> -->
      <!-- <a href="mailto:jiangsongyao@gmail.com"><i class="fas fa-fw fa-envelope-square"></i></a>
      <a href="doc/CV_Songyao_Jiang_20210901.pdf" target="_blank"><i class="ai fa-fw ai-cv-square"></i></a>
      <a href="https://scholar.google.com/citations?user=Q5E2uoMAAAAJ" target="_blank"><i class="fas fa-fw fa-graduation-cap"></i></a>
      <a href="https://github.com/jackyjsy" target="_blank"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      <a href="https://www.linkedin.com/in/songyao-jiang-739816149/" target="_blank"><i class="fab fa-fw fa-linkedin"></i></a>  -->
    </div>

  <!-- <table width="900" border="0" align="center" cellspacing="0" cellpadding="0" style="padding-left: 140px;"> -->
  <!-- <table style="width:100%;max-width:900px;min-width:600px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-left:140px;"> -->
    <table class="all">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <a class="anchor" id="about"></a>
      <tr>
        <td class="hero-top">
          <img src="img/songyao_jiang_2022.png">
        </td>
      </tr>
      <tr>
        <td class="hero-left" valign="middle">
        <p align="center">
          <name>Songyao Jiang</name>
          <!-- <name class="chinesename"> 姜淞耀</name> -->
        </p>
        <p>
          I am an Applied Scientist at <a href="https://amazon.jobs/en/teams/lab126/">Amazon Lab126</a>, where I work on computer vision and machine learning algorithms for Amazon devices. 
          My current research interests lie in human pose estimation, object detection, and sign language recognition.
        </p>
        <p>I received my Ph.D. in Computer Engineering at <a href="http://www.northeastern.edu/">Northeastern University</a>, 
          <a href="https://web.northeastern.edu/smilelab/">SmileLab</a> 
          advised by <a href="http://www1.ece.neu.edu/~yunfu/">Dr. Yun (Raymond) Fu</a>. I received my masters degree at the <a href="https://www.umich.edu/">University of Michigan</a> 
          and my bachelors at <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>. 
        </p>
        <!-- <p>
          My current research interests lie in computer vision and deep learning. I am good at applying deep learning to real-world problems. 
        </p> -->
        <p>I was the team leader when we won 
          <a href="http://chalearnlap.cvc.uab.es/challenge/43/description/"> the CVPR 2021 Challenge on Large Scale Signer Independent Isolated Sign Language Recognition </a>. 
          Our team (smilelab2021) ranked 1st in both RGB and RGB-D tracks.
        </p>
        <p>I am also an innovative entrepreneur. I paticipated in <a href="https://www.nsf.gov/news/special_reports/i-corps/">NSF I-Corps</a> and <a href="https://www.northeastern.edu/cri/resources/i-corps/">NSF I-Corps Sites at NEU</a> as Enterpreneurial Leads. I was core founding members of two AI startup companies: Giaran, Inc. and <a href="https://ainnovationlabs.com/">AInnovation Labs, Inc.</a> 
          Giaran was acquired by <a href="https://www.shiseido.com/">Shiseido Americas</a> in Nov. 2017 <a href="https://www.businesswire.com/news/home/20171107006109/en/Shiseido-Americas-Acquires-Giaran/">[News]</a>.
        </p>
        <p>
          <!-- At Google I've worked on <a href="http://googleresearch.blogspot.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="http://googleresearch.blogspot.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://research.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. -->
        </p>
        <p>
          Beyond academics, I am also a skilled astronomy and landscape photographer, and here is my <a href="http://photography.songyaojiang.com">Little Gallery</a>.
        </p>
        <!-- <p>
          I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've spent time at <a href="https://en.wikipedia.org/wiki/Google_X">Google[x]</a>, <a href="http://groups.csail.mit.edu/vision/welcome/">MIT CSAIL</a>, <a href="http://www.captricity.com/">Captricity</a>, <a href="https://www.nasa.gov/ames">NASA Ames</a>, <a href="http://www.google.com/">Google NYC</a>, the <a href="http://mrl.nyu.edu/">NYU MRL</a>, <a href="http://www.nibr.com/">Novartis</a>, and <a href="http://www.astrometry.net/">Astrometry.net</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
        </p> -->
        <p align=center>
          <a href="mailto:jiangsongyao@gmail.com"><i class="fas fa-fw fa-envelope"></i> Email</a> &nbsp/&nbsp
          <a href="doc/CV_Songyao_Jiang_202302.pdf" target="_blank"><i class="fas fa-fw fa-file-alt"></i> CV</a> &nbsp/&nbsp
          <!-- <a href="doc/SongyaoJiang-bio.txt">Biography</a> &nbsp/&nbsp -->
          <a href="https://scholar.google.com/citations?user=Q5E2uoMAAAAJ" target="_blank"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar </a> &nbsp/&nbsp
          <a href="https://github.com/jackyjsy" target="_blank"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/songyao-jiang-739816149/" target="_blank"><i class="fab fa-fw fa-linkedin"></i> LinkedIn </a> 
          <!-- &nbsp/&nbsp -->
          <!-- <a href="http://photography.songyaojiang.com"> Gallery </a> &nbsp/&nbsp -->
          <!-- <a href="https://syjiangblog.wordpress.com/"> Blog </a> -->
        </p>
        </td>
        <td class="hero-right">
          <img src="img/songyao_jiang_2022.png" width="100%">
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <a class="anchor" id="news"></a>
            <heading>News</heading>
            <!-- <p> -->
              <ul>
                <li>[11/2022] One paper is accepted to AAAI 2022.</li>
                <li>[06/2022] Join Amazon Lab126 as Applied Scientist.</li>
                <li>[04/2022] Successfully defend my Ph.D. dissertation.</li>
                <li>[04/2022] Participate in <a href="https://www.nsf.gov/news/special_reports/i-corps/teams.jsp">NSF I-Corps Spring 2022</a> as Entrepreneurial Lead.</li>
                <li>[12/2021] Be awarded NVIDIA CCS <a href="doc/best_student_paper.pdf">Best Student Paper</a>. </li>
                <li>[11/2021] One oral paper is accepted to FG 2021. </li>
                <li>[10/2021] One patent is published: WO/2021/163103. 
                  <a href="https://patents.google.com/patent/WO2021163103A1/">[link]</a>
                </li>
                <li>[07/2021] One paper is accepted to ACM MM 2021.</li>
                <li>[06/2021] Our team ranked the 4th place in CVPR21 Challenge on Agriculture Vision. 
                  <a href="https://github.com/jackyjsy/CVPR21Chal-Agrivision">[code]</a> 
                  <a href="https://www.agriculture-vision.com/agriculture-vision-2021/prize-challenge-2021">[link]</a> 
                  <a href="https://competitions.codalab.org/competitions/30690?secret_key=cd446fcd-0ed2-4e51-bd9f-f0da9a4c472a#results">[CodaLab]</a> 
                </li>
                <li>[05/2021] One paper is accepted to CVPR 2021 Workshops.</li>
                <li>[04/2021] Our team won the <strong>championships</strong> in both RGB & RGB-D tracks of CVPR21 Challenge on Large Scale Signer Independent Isolated Sign Language Recognition. 
                  <a href="https://github.com/jackyjsy/CVPR21Chal-SLR">[code]</a> 
                  <a href="https://chalearnlap.cvc.uab.cat/challenge/43/description/">[link]</a> 
                  <a href="https://coe.northeastern.edu/news/smile-lab-wins-1st-at-conference-on-computer-vision-and-pattern-recognition/">[news]</a>
                </li>
                <li>[01/2021] One paper is accepted to IEEE TIP.</li>
                <li>[12/2020] One paper is accepted to Nature: Commun. Biol.</li>
                <li>[11/2020] One patent is published: WO/2020/232069.
                  <a href="https://patents.google.com/patent/WO2020232069A1/en">[link]</a>
                </li>
                <li>[10/2020] One patent is granted: US Patent 10,825,219.
                  <a href="https://patents.google.com/patent/US10825219B2/en/">[link]</a>
                </li>
                <li>[03/2020] One paper is accepted by FG 2020.</li>
                <li>[02/2020] One paper is accepted to Lab on a Chip.</li>
                <li>[01/2020] One patent is published: US/2020/0015575.
                  <a href="https://patents.google.com/patent/US20200015575A1/en">[link]</a>
                </li>
                <li>[03/2019] One paper is accepted to FG 2019.</li>
                <li>[01/2019] Receive GapFund360 Award.
                  <a href="https://www.northeastern.edu/cri/news/gapfund360-awardees-announced/">[news]</a>
                </li>
                <li>&hellip;</li>
              </ul>
            <!-- </p> -->

          </td>
        </tr>
      <tr>
        <td width="100%" valign="middle">
          <a class="anchor" id="research"></a>
          <heading>Research</heading>
          <p>
          I'm interested in computer vision, machine learning, image processing, and computational photography. 
          My current research includes whole-body pose estimation, skeleton-based sign language recognition and action recognition.
          </p>
        </td>
      </tr>
      </table>

  <table class="researchblock" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr class="research">
      <td width="30%">
          <a href="img/pixel-unshuffle.jpg"><img src='img/pixel-unshuffle.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://arxiv.org/abs/2203.08921">
          <papertitle>Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution</papertitle></a><br>
          Bin Sun, Yulun Zhang, <strong>Songyao Jiang</strong>, and Yun Fu<br>
          <em>AAAI</em>, 2023<br>
          <a href="https://arxiv.org/abs/2203.08921">Paper </a> /
          <a href="https://github.com/Sun1992/HPUN">GitHub </a> /
          <a href="https://user-images.githubusercontent.com/15020820/203320884-a6756cc4-492e-445b-aa63-79e4ee6cfd21.MP4">Demo</a>
          <p> 
            Downsampling features for multi-resolution fusion is an efficient and effective way to improve the performance of visual recognition. 
            Still, it is counter-intuitive in the SR task, which needs to project a low-resolution input to high-resolution. In this paper, 
            we propose a novel Hybrid Pixel-Unshuffled Network (HPUN) by introducing an efficient and effective downsampling module into the SR task. 
          </p>
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/sam-slr-v2.jpg"><img src='img/sam-slr-v2.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://github.com/jackyjsy/SAM-SLR-v2">
          <papertitle>Sign Language Recognition via Skeleton Aware Multi-modal Ensemble (SAM-SLR-v2)</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu<br>
          <em>Under review</em>, 2021<br>
          <a href="https://arxiv.org/abs/2110.06161">Paper </a> / 
          <a href="https://github.com/jackyjsy/sam-slr-v2">GitHub</a><br>
          <p> 
            This paper is an extension of our previous work SAM-SLR that won the sign language recognition challenge in CVPR 2021. 
            Our major improvements include introducing a new Keypoint3D modality that improves the ensemble accuracy in the RGB-D scenario. 
            proposing a Global Ensemble Model to automatically learn multi-modal ensemble,  
            and providing extensive experiments on three major SLR datasets. 
            Our approach achieves the state-of-the-art performance with significant margins.
          </p>
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
          <a href="img/sam-slr_large.jpg"><img src='img/sam-slr.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://github.com/jackyjsy/CVPR21Chal-SLR">
          <papertitle>Skeleton Aware Multi-modal Sign Language Recognition (SAM-SLR)</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu<br>
          <em>Championships winner of the <a href="http://chalearnlap.cvc.uab.es/challenge/43/description/">CVPR21 Challenge on Sign Language Recognition</a></em><br>
          <em>CVPR21 Workshop</em>, 2021<br>
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Jiang_Skeleton_Aware_Multi-Modal_Sign_Language_Recognition_CVPRW_2021_paper.pdf"> Paper </a> / 
          <a href="https://github.com/jackyjsy/CVPR21Chal-SLR">GitHub</a> / 
          <a href="https://youtu.be/S0VEYnle8SA">YouTube</a> /
          <a href="https://competitions.codalab.org/competitions/27901#results"> RGB Leaderboard</a> / 
          <a href="https://competitions.codalab.org/competitions/27902#results">RGB-D Leaderboard</a> <br>
          <p> 
            We proposed a skeleton-aware multi-modal sign language recognition framework (SAM-SLR) to capture information from multiple modalities
            and assemble them together to further boost the performance. Our team ranked 1st in both RGB and RGB-D tracks in the challenge. 
          </p>
      </td>
    </tr>
    <tr>
      <td width="30%" align="center" >
          <a href="img/agrivision.gif"><img src='img/agrivision.gif' width="85%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://github.com/jackyjsy/CVPR21Chal-Agrivision">
          <papertitle>Multi-label Semantic Segmentation for Multi-modal Agricultural Pattern Recognition</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Bin Sun, and Yun Fu<br>
          <em>Ranked 4th in the CVPR21 Challenge on the Agriculture-Vision</em>, 2021<br>
          
          <a href="https://github.com/jackyjsy/CVPR21Chal-Agrivision">GitHub</a> / 
          <a href="https://competitions.codalab.org/competitions/30690?secret_key=cd446fcd-0ed2-4e51-bd9f-f0da9a4c472a#results">Leaderboard</a><br>
          <p> 
            We developed a framework consisting a GCN based and a DeepLabv3 based multi-label semantic segmentation models to recognize the agricultural patterns (e.g. cloud shadow, double plant, standing water and weed cluster) in multi-modal RGB and NIR aerospace images. Our final result achieved 0.507 mIoU. 
          </p>
          
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
          <a href="img/TIP_GEGAN.jpg"><img src='img/TIP_GEGAN.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://ieeexplore.ieee.org/abstract/document/9336328">
          <papertitle>Geometrically Editable Face Image Translation With Adversarial Networks</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Zhiqiang Tao, and Yun Fu<br>
          <em>IEEE Transactions on Image Processing (TIP)</em>, 2021<br>
          <a href="https://ieeexplore.ieee.org/abstract/document/9336328">Paper</a> / 
          <a href="https://github.com/"> GitHub </a><br>
          <p> 
            We formulate the image translation problem as multi-domain mappings in both geometric and attribute 
            directions and propose a novel Geometrically Editable Generative Adversarial Networks (GEGAN) model to learn such mappings of geometric editable translations.
          </p>
          
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/image_generation_large.jpg"><img src='img/image_generation.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="https://jackyjsy.github.io/SCGAN/">
          <papertitle>Spatially Constrained GAN for Face and Fashion Synthesis</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Hongfu Liu, Yue Wu and Yun Fu<br>
          <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</em>2021<br>
          <em>NVIDIA CCS Best Student Paper Award</em><br>
          <a href="https://arxiv.org/abs/1905.02320">Paper</a> / 
          <a href="https://github.com/jackyjsy/SCGAN">GitHub</a> /
          <a href="doc/best_student_paper.pdf">Award</a> /
          <a href="https://jackyjsy.github.io/SCGAN/">Website</a><br>
          <p>Image generation has raised tremendous attention
              in both academic and industrial areas, especially
              for criminal portrait and fashion design.
              We propose a novel Spatially Constrained Generative Adversarial Network
              , which decouples the spatial constraints from
              the latent vector and makes them feasible as
              additional controllable signals. </p>
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
          <a href="img/commsbio_large.jpg"><img src='img/commsbio.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <!-- <a href="https://ieeexplore.ieee.org/abstract/document/9336328"> -->
          <papertitle>Harnessing Deep Learning to Overcome Photo-toxicity for Live-cell Imaging</papertitle></a><br>
          Zhenmin Hong, Tengqian Sun, <strong>Songyao Jiang</strong>, Kunpeng Li, Yun Fu, Huizhong Xu, Jin Zhang, Yongmin Liu, Qing Ye, Hu Cang<br>
          <!-- <em>Nature: Communnications Biology</em>, 2021<br> -->
          <em>Under review</em>, 2021<br>
          <!-- Paper / 
           GitHub <br> -->
          <p> 
            Photo-toxicity is a long-standing challenge in live-cell imaging, which damages macromolecules and organelles,
            impairs biological processes, and even compromises cells, leading to cell death. We demonstrate a deep-learning
            strategy to minimize photodamages and enable high-speed imaging of transient cellular events that are difficult to capture.
          </p>
          
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/superfront.JPG"><img src='img/superfront.JPG' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://arxiv.org/abs/2012.04111">
          <papertitle>SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis</papertitle></a><br>
          Yu Yin, Joseph P. Robinson, <strong>Songyao Jiang</strong>, Yue Bai, Can Qin and Yun Fu<br>
          <em>ACM Multimedia (ACMMM)</em>, 2021<br>
          <a href="https://arxiv.org/abs/2012.04111">Paper</a> / <a href="https://github.com/YuYin1/SuperFront">GitHub</a><br>
          <p> Existing face frontalization methods tend to focus on samples with variation in pose, but with the assumption data 
            is high in quality. We propose a generative adversarial network (GAN) -based model to generate high-quality, 
            identity preserving frontal faces from one or multiple low-resolution (LR) faces with extreme poses. 
          </p>
          
      </td>
    </tr>
    <tr>
    <tr class="research">
      <td width="30%">
          <a href="img/face_frontalization_large.jpg"><img src='img/face_frontalization.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://github.com/YuYin1/DA-GAN">
          <papertitle>Dual-Attention GAN for Large-Pose Face Frontalization</papertitle></a><br>
          Yu Yin, <strong>Songyao Jiang</strong>, Joseph P. Robinson, and Yun Fu<br>
          <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</em>, 2020<br>
          <a href="https://arxiv.org/abs/2002.07227">Paper</a> / 
          <a href="https://github.com/YuYin1/DA-GAN"> GitHub </a><br>
          <p> Face frontalization provides an effective and efficient way for face data augmentation and further improves the
            face recognition performance in extreme pose scenario. We present a novel
            Dual-Attention Generative Adversarial Network (DA-GAN) for
            photo-realistic face frontalization by capturing both contextual
            dependencies and local consistency during GAN training. 
          </p>
          
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/labonchip_2.jpg"><img src='img/labonchip_1.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://pubs.rsc.org/no/content/articlelanding/2020/lc/d0lc00158a/unauth#!divAbstract">
          <papertitle>Machine Learning-aided Quantification of Antibody-based Cancer Immunotherapy by Natural Killer Cells in Microfluidic Droplets</papertitle><br>
          </a>
          Saheli Sarkar, Wenjing Kang, <strong>Songyao Jiang</strong>, Kunpeng Li, Somak Ray, Ed Luther, Alexander R Ivanov, Yun Fu, and Tania Konry<br>
          <em>Lab on a Chip</em>, 2020<br>
          <!-- <a href="https://pubs.rsc.org/no/content/articlelanding/2020/lc/d0lc00158a/unauth#!divAbstract">Paper</a><br> -->
          <p> Natural killer (NK) cells have emerged as an effective alternative option to T cell-based immunotherapies, 
            particularly against liquid (hematologic) tumors. This paper describes a microfluidic 
            droplet-based cytotoxicity assay for quantitative comparison of immunotherapeutic NK-92 cell interaction with various types of target cells. 
            Machine learning algorithms were developed to assess the dynamics of individual effector-target cell pair conjugation and target death in droplets in a semi-automated manner. 
          </p>
          
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
          <a href="img/pose_estimate_large.jpg"><img src='img/pose_estimate.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
          <a href="https://patents.google.com/patent/WO2020232069A1/en">
          <papertitle>Video-based Multi-person Pose Estimation and Tracking</papertitle></a><br>
          <strong>Songyao Jiang</strong>, and Yun Fu<br>
          <em>Patent Published WO/2020/232069</em>, 2020<br>
          <a href="https://patents.google.com/patent/WO2020232069A1/en">Patent</a> / 
          <a href="https://github.com/jackyjsy/MultiPoseInferene">GitHub</a><br>
          <p> Our invention aims to tackle the video-based multi-person pose estimation problem using a deep learning framework 
            with multi-frame refinement and optimization. Our method inherently tracks the estimated poses 
            and makes the model insensitive to occlusions. Moreover, we reduce the shaking and vibrations phenomenon of the estimated pose skeletons in video pose estimation.
          </p>
      </td>
    </tr>
    <tr>
      <td width="30%">
          <a href="img/image_translation_large.jpg"><img src='img/image_translation.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/1901.01569">
          <papertitle>Segmentation Guided Image-to-Image Translation with Adversarial Networks</papertitle></a><br>
          <strong>Songyao Jiang</strong>, Zhiqiang Tao and Yun Fu<br>
          <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</em>, 2019 <br>
          <a href="https://arxiv.org/abs/1901.01569">Paper</a> / 
          <a href="https://github.com/jackyjsy/SGGAN">GitHub</a> / 
          <a href="https://arxiv.org/abs/1901.01569">ArXiv</a><br>
          <p> Recently image-to-image translation methods neglect to utilize higher-level and instance-specific
              information to guide the training process, leading to a great
              deal of unrealistic generated images of low quality. We propose a novel Segmentation
              Guided Generative Adversarial Networks, which
              leverages semantic segmentation to further boost the generation
              performance and provide spatial mapping. </p>
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
          <a href="img/domain_adapt.jpg"><img src='img/domain_adapt.jpg' width="100%"></a>
      </td>
      <td valign="top" width="75%">
        <p>
        <!-- <p><a href="https://"> -->
          <papertitle>Face Recognition and Verification in Low-light Condition</papertitle><br>
          <strong>Songyao Jiang</strong>, Yue Wu, Zhengming Ding, and Yun Fu<br>
          <!-- <em></em>2020<br> -->
          <!-- Paper / 
          GitHub<br> -->
          <p> This project tends to solve the problem of recognizing people in low light condition, 
            which is quite useful in security. In low-light condition, we usually utilize near IR, 
            mid-range IR and long-range IR to obtain the portrait images of the target person. 
            In our low-light face recognition and verification system, we developed a semi-supervised and an unsupervised 
            transfer learning methods to transfer the knowledge we learned from visible spectrum to 
            IR spectrum. . 
          </p>
          
      </td>
    </tr>
    <tr>
      <td width="30%">
        <a href="img/rule-based.jpg"><img src='img/rule-based.jpg' width="100%" alt="rule-based"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://ieeexplore.ieee.org/document/7961759">
          <papertitle> Rule-Based Facial Makeup Recommendation System </papertitle></a><br>
          Taleb Alashkar, <strong>Songyao Jiang</strong> and Yun Fu<br>
          <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</em>, 2017 <br>
          <!-- <a href="https://ieeexplore.ieee.org/document/7961759">Paper</a> / 
          GitHub<br> -->
          <p>An automatic and smart facial makeup recommendation 
            and synthesis system is proposed. An automatic facial 
            makeup synthesis system is aslo developed to apply the recommended style 
            on the facial image as well. A new dataset with 961 different females photos 
            collected and labeled. </p>
      </td>
    </tr>
    <tr class="research">
      <td width="30%">
        <a href="img/makeup-recommend.png"><img src='img/makeup-recommend.png' width="100%" alt="makeup-recommend"></a>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://dl.acm.org/doi/10.5555/3298239.3298377">
          <papertitle>  Examples-Rules Guided Deep Neural Network for Makeup Recommendation</papertitle></a><br>
          Taleb Alashkar, <strong>Songyao Jiang</strong>, Shuyang Wang and Yun Fu<br>
          <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2017 <br>
          <!-- <a href="https://dl.acm.org/doi/10.5555/3298239.3298377">Paper</a> / 
          GitHub<br> -->
          <p> We consider a fully automatic makeup recommendation system and propose a novel 
            examples-rules guided deep neural network approach. Makeup-related facial traits are classified into structured 
            coding. These facial traits are fed in- to examples-rules guided deep 
            neural recommendation model which makes use of the pairwise of Before-After images 
            and the makeup artist knowledge jointly. To visualize the recommended makeup 
            style, an automatic makeup synthesis system is developed as well. </p>
      </td>
    </tr>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <a class="anchor" id="pub"></a>
          <heading>Publications</heading>
          <ul>
            <li>B. Sun, Y. Zhang, <strong>S. Jiang</strong>, and Y. Fu, “Hybrid Pixel-Unshuffled Network for Lightweight Image Super-Resolution,”<br/>
              <i>Under Review</i>, 2021.</li>
            <li><strong>S. Jiang</strong>, B. Sun, L. Wang, Y. Bai, K. Li, and Y. Fu, “Sign Language Recognition via Skeleton-aware Multi-modal Ensemble,”<br/>
              <i>Under Review</i>, 2021.</li>
            <li><strong>S. Jiang</strong>, B. Sun, L. Wang, Y. Bai, K. Li, and Y. Fu, “Skeleton Aware Multi-modal Sign Language Recognition,”<br/>
              in <i>Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</i>, 2021.</li>
            <li><strong>S. Jiang</strong>, Z. Tao, and Y. Fu, “Geometrically Editable Face Image Translation with Adversarial Networks,”<br/>
              <i>IEEE Transactions on Image Processing (TIP)</i>, 2021.</li>
            <li><strong>S. Jiang</strong>, H. Liu, Y. Wu, and Y. Fu, “Spatially Constrained GAN for Face and Fashion Synthesis,”<br/>
              in <i>Proceedings of 16th IEEE International Conference on Automatic Face & Gesture Recognition (FG)</i>, 2021.</li>
            <li>Y. Yin, J. P. Robinson, <strong>S. Jiang</strong>, and Y. Fu, “SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis,”<br/>
              in <i>Proceedings of ACM Multimedia (ACMMM)</i>, 2021.</li>
            <li>Y. Yin, <strong>S. Jiang</strong>, J. P. Robinson, and Y. Fu, “Dual-attention GAN for Large-pose Face Frontalization,”<br/>
              in <i>Proceedings of 15th IEEE International Conference on Automatic Face & Gesture Recognition (FG)</i>, 2020.</li>
            <li>S. Sarkar, W. Kang, <strong>S. Jiang</strong>, K. Li, S. Ray, E. Luther, ... and T. Konry, “Machine learning-aided quantification of antibody-based cancer immunotherapy by natural killer cells in microfluidic droplets,”<br/>
              <i>Lab on a Chip, 20(13), pp. 2317-2327</i>, 2020.</li>
            <li>Z. Hong, T. Sun, <strong>S. Jiang</strong>, K. Li, Y. Fu, H. Xu, J. Zhang, Y. Liu, Q. Ye, and H. Cang, “Harnessing Deep Learning to Overcome Photo-toxicity for Live-cell Imaging,” 
              <i>Under Review</i>, 2020.</li>
            <li><strong>S. Jiang</strong>, Z. Tao, and Y. Fu, “Segmentation Guided Image-to-Image Translation with Adversarial Networks,”<br/>
              in <i>Proceedings of 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG), </i>, 2019.</li>
            <li>T. Alashkar, <strong>S. Jiang</strong>, and Y. Fu, “Rule-Based Facial Makeup Recommendation System,”<br/>
              in <i>Proceedings of 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG), </i>, 2017.</li>
            <li>T. Alashkar, <strong>S. Jiang</strong>, S. Wang, and Y. Fu, “Examples-Rules Guided Deep Neural Network for Makeup Recommendation,”<br/>
              in <i>Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</i>, 2017.</li>       
            <li><strong>S. Jiang</strong>, and T. Kato, “Dynamic Modelling of Combined Cycle Power Plant for Load Frequency Control with Large Penetration of Renewable Energy,” 
              in <i>7th JUACEP Workshop</i>, 2014.</li>    
          </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <a class="anchor" id="patents"></a>
          <heading>Patents</heading>
          <ul>
            <li>Y. Fu, <strong>S. Jiang</strong>, “Segmentation Guided Image Generation with Adversarial Networks”.<br/>
              <i>US Patent 10,825,219.</i></li>
            <li>Y. Fu, <strong>S. Jiang</strong>, B. Sun, “Light-Weight Pose Estimation Network with Multi-Scale Heatmap Fusion”.<br/>
              <i>US Patent App. No.: 62/976,099.</i></li>
            <li>Y. Fu, <strong>S. Jiang</strong>, “Video 2D Multi-person Pose Estimation using Multi-frame Refinement and Optimization”.<br/>
              <i>WIPO Patent App. No.: WO 2020/232069.</i></li>
            <li>Y. Fu, S. Wang, S. Lee, <strong>S. Jiang</strong>, B. Sun, H. Mao, K. H. E. Cheung, “Systems and Methods for Virtual Facial 
              Makeup Removal and Simulation, Fast Facial Detection and Landmark Tracking, Reduction in … ”.<br/>
              <i>US Patent App. No: 16/584,310.</i></li>
          </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <a class="anchor" id="awards"></a>
          <heading>Honors & Awards</heading>
          <ul>
            <li>NVIDIA CCS Best Student Paper Award, FG 2021.</li>
            <li>Champions of CVPR21 Challenge on Large-Scale Signer-independent Isolated Sign Language Recognition (RGB & RGB-D tracks), 2021.</li>
            <li>4th Rank in CVPR21 Challenge on Agriculture-Vision (supervised track), 2021.</li>
            <li>PhD Network Travel Grant, Northeastern University, USA, 2019.</li>
            <li>GapFund360 Award, Northeastern University, USA, 2018.</li>
            <li>NSF I-Corps Grant, National Science Foundation, 2016.</li>
            <li>JUACEP Research Award, Nagoya University, Japan, 2014.</li>
            <li>JASSO Scholarship, Nagoya University, Japan, 2014</li>
            <li>Outstanding Scholarship, Hong Kong Polytechnic University, Hong Kong, 2010, 2011, 2012, 2013.</li>
          </ul>
        </td>
      </tr>
      <tr>
        <td width="100%" valign="middle">
          <a class="anchor" id="service"></a>
          <heading>Academic Service</heading>
          <ul><strong>Conference PC Member and Reviewer:</strong>
          <li>International Conference on Computer Vision (ICCV)</li>
          <li>International Joint Conferences on Artificial Intelligence (IJCAI)</li>
          <li>IEEE International Conference on Automatic Face & Gesture Recognition (FG)</li>
          <li>IEEE International Conference on Data Mining (ICDM)</li>
          <li>IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR)</li>
          </ul>
          <ul><strong>Journal Reviewer:</strong>
            <li>IEEE Transactions on Image Processing (TIP)</li>
            <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
            <li>Journal of Visual Communication and Image Representation (JVCI)</li>
            <li>The Vision Computer (TVCJ)</li>
            <li>IET Image Processing</li>
            <li>Journal of Electronic Imaging (JEI)</li>
          </ul>
          <ul><strong>Workshop Reviewer:</strong>
            <li>IEEE International Workshop on Analysis and Modeling of Faces and Gestures Workshops (AMFG)</li>
          </ul>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          This website is generated using source code from <a target="_blank" href="https://jonbarron.info/"><strong>Jon Barron</strong></a>.
	        </font>
        </p>
        </td>
      </tr>
      
    </table>
    <table width="10%" align="center" border="0" cellspacing="0" cellpadding="20">
      <script type="text/javascript" id="clstr_globe" src="https://cdn.clustrmaps.com/globe.js?d=C02jbGLeKNPfv3nMc0a1D-t73HzskgbBQUPgjzvtT4c"></script>
    </table>
    </td>
    </tr>
  </table>
  </body>
</html>
